{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-01T04:29:37.216322Z",
     "start_time": "2024-10-01T04:29:37.212743Z"
    }
   },
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:29:42.489921Z",
     "start_time": "2024-10-01T04:29:38.230136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install BeautifulSoup4\n",
    "!pip install pydot"
   ],
   "id": "caeba049b7e7571a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: BeautifulSoup4 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from BeautifulSoup4) (2.5)\n",
      "Collecting pydot\n",
      "  Downloading pydot-3.0.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pyparsing>=3.0.9 in c:\\users\\yc\\.conda\\envs\\sdptest\\lib\\site-packages (from pydot) (3.1.2)\n",
      "Downloading pydot-3.0.2-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: pydot\n",
      "Successfully installed pydot-3.0.2\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:32:48.360008Z",
     "start_time": "2024-10-01T04:32:48.318914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter,defaultdict\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score,StratifiedKFold,KFold,StratifiedShuffleSplit,cross_val_predict\n",
    "from lightgbm import LGBMClassifier as lg\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D,GlobalMaxPool1D\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import keras.backend as k\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional,GRU\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# from kaggle_datasets import KaggleDatasets\n",
    "import transformers\n",
    "from tqdm.notebook import tqdm\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, pipeline, TFDistilBertModel,TFAutoModel\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ],
   "id": "5aa24190ecdf8780",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.preprocessing.text'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 24\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras\n\u001B[1;32m---> 24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Tokenizer\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D,GlobalMaxPool1D\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptimizers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Adam\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'keras.preprocessing.text'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "18b6f47ac7c42ffb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:35:03.528573Z",
     "start_time": "2024-10-01T04:35:01.249407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df= pd.read_json(\"resources/embold_train.json\").reset_index(drop=True)\n",
    "test_df= pd.read_json(\"resources/embold_test.json\").reset_index(drop=True)\n",
    "train_extra_df= pd.read_json(\"resources/embold_train_extra.json\").reset_index(drop=True)"
   ],
   "id": "776d37bac2a501c3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:41:03.422422Z",
     "start_time": "2024-10-01T04:41:02.214438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fx(x):\n",
    "    return x['title'] + \" \" + x['body']   \n",
    "train_df['text']= train_df.apply(lambda x : fx(x),axis=1)\n",
    "test_df['text']= test_df.apply(lambda x : fx(x),axis=1)"
   ],
   "id": "b72cda1201374f92",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:41:05.105707Z",
     "start_time": "2024-10-01T04:41:05.092244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cList = {\n",
    "            \"i'm\": \"i am\",\n",
    "            \"you're\": \"you are\",\n",
    "            \"it's\": \"it is\",\n",
    "            \"we're\": \"we are\",\n",
    "            \"we'll\": \"we will\",\n",
    "            \"That's\":\"that is\",\n",
    "            \"haven't\":\"have not\",\n",
    "            \"let's\":\"let us\",\n",
    "            \"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "            \"aren't\": \"are not / am not\",\n",
    "            \"can't\": \"cannot\",\n",
    "            \"can't've\": \"cannot have\",\n",
    "            \"'cause\": \"because\",\n",
    "            \"could've\": \"could have\",\n",
    "            \"couldn't\": \"could not\",\n",
    "            \"couldn't've\": \"could not have\",\n",
    "            \"didn't\": \"did not\",\n",
    "            \"doesn't\": \"does not\",\n",
    "            \"don't\": \"do not\",\n",
    "            \"hadn't\": \"had not\",\n",
    "            \"hadn't've\": \"had not have\",\n",
    "            \"hasn't\": \"has not\",\n",
    "            \"haven't\": \"have not\",\n",
    "            \"he'd\": \"he had / he would\",\n",
    "            \"he'd've\": \"he would have\",\n",
    "            \"he'll\": \"he shall / he will\",\n",
    "            \"he'll've\": \"he shall have / he will have\",\n",
    "            \"he's\": \"he has / he is\",\n",
    "            \"how'd\": \"how did\",\n",
    "            \"how'd'y\": \"how do you\",\n",
    "            \"how'll\": \"how will\",\n",
    "            \"how's\": \"how has / how is / how does\",\n",
    "            \"I'd\": \"I had / I would\",\n",
    "            \"I'd've\": \"I would have\",\n",
    "            \"I'll\": \"I shall / I will\",\n",
    "            \"I'll've\": \"I shall have / I will have\",\n",
    "            \"I'm\": \"I am\",\n",
    "            \"I've\": \"I have\",\n",
    "            \"isn't\": \"is not\",\n",
    "            \"it'd\": \"it had / it would\",\n",
    "            \"it'd've\": \"it would have\",\n",
    "            \"it'll\": \"it shall / it will\",\n",
    "            \"it'll've\": \"it shall have / it will have\",\n",
    "            \"it's\": \"it has / it is\",\n",
    "            \"let's\": \"let us\",\n",
    "            \"ma'am\": \"madam\",\n",
    "            \"mayn't\": \"may not\",\n",
    "            \"might've\": \"might have\",\n",
    "            \"mightn't\": \"might not\",\n",
    "            \"mightn't've\": \"might not have\",\n",
    "            \"must've\": \"must have\",\n",
    "            \"mustn't\": \"must not\",\n",
    "            \"mustn't've\": \"must not have\",\n",
    "            \"needn't\": \"need not\",\n",
    "            \"needn't've\": \"need not have\",\n",
    "            \"o'clock\": \"of the clock\",\n",
    "            \"oughtn't\": \"ought not\",\n",
    "            \"oughtn't've\": \"ought not have\",\n",
    "            \"shan't\": \"shall not\",\n",
    "            \"sha'n't\": \"shall not\",\n",
    "            \"shan't've\": \"shall not have\",\n",
    "            \"she'd\": \"she had / she would\",\n",
    "            \"she'd've\": \"she would have\",\n",
    "            \"she'll\": \"she shall / she will\",\n",
    "            \"she'll've\": \"she shall have / she will have\",\n",
    "            \"she's\": \"she has / she is\",\n",
    "            \"should've\": \"should have\",\n",
    "            \"shouldn't\": \"should not\",\n",
    "            \"shouldn't've\": \"should not have\",\n",
    "            \"so've\": \"so have\",\n",
    "            \"so's\": \"so as / so is\",\n",
    "            \"that'd\": \"that would / that had\",\n",
    "            \"that'd've\": \"that would have\",\n",
    "            \"that's\": \"that has / that is\",\n",
    "            \"there'd\": \"there had / there would\",\n",
    "            \"there'd've\": \"there would have\",\n",
    "            \"there's\": \"there has / there is\",\n",
    "            \"they'd\": \"they had / they would\",\n",
    "            \"they'd've\": \"they would have\",\n",
    "            \"they'll\": \"they shall / they will\",\n",
    "            \"they'll've\": \"they shall have / they will have\",\n",
    "            \"they're\": \"they are\",\n",
    "            \"they've\": \"they have\",\n",
    "            \"to've\": \"to have\",\n",
    "            \"wasn't\": \"was not\",\n",
    "            \"we'd\": \"we had / we would\",\n",
    "            \"we'd've\": \"we would have\",\n",
    "            \"we'll\": \"we will\",\n",
    "            \"we'll've\": \"we will have\",\n",
    "            \"we're\": \"we are\",\n",
    "            \"we've\": \"we have\",\n",
    "            \"weren't\": \"were not\",\n",
    "            \"what'll\": \"what shall / what will\",\n",
    "            \"what'll've\": \"what shall have / what will have\",\n",
    "            \"what're\": \"what are\",\n",
    "            \"what's\": \"what has / what is\",\n",
    "            \"what've\": \"what have\",\n",
    "            \"when's\": \"when has / when is\",\n",
    "            \"when've\": \"when have\",\n",
    "            \"where'd\": \"where did\",\n",
    "            \"where's\": \"where has / where is\",\n",
    "            \"where've\": \"where have\",\n",
    "            \"who'll\": \"who shall / who will\",\n",
    "            \"who'll've\": \"who shall have / who will have\",\n",
    "            \"who's\": \"who has / who is\",\n",
    "            \"who've\": \"who have\",\n",
    "            \"why's\": \"why has / why is\",\n",
    "            \"why've\": \"why have\",\n",
    "            \"will've\": \"will have\",\n",
    "            \"won't\": \"will not\",\n",
    "            \"won't've\": \"will not have\",\n",
    "            \"would've\": \"would have\",\n",
    "            \"wouldn't\": \"would not\",\n",
    "            \"wouldn't've\": \"would not have\",\n",
    "            \"y'all\": \"you all\",\n",
    "            \"y'all'd\": \"you all would\",\n",
    "            \"y'all'd've\": \"you all would have\",\n",
    "            \"y'all're\": \"you all are\",\n",
    "            \"y'all've\": \"you all have\",\n",
    "            \"you'd\": \"you had / you would\",\n",
    "            \"you'd've\": \"you would have\",\n",
    "            \"you'll\": \"you shall / you will\",\n",
    "            \"you'll've\": \"you shall have / you will have\",\n",
    "            \"you're\": \"you are\",\n",
    "            \"you've\": \"you have\"\n",
    "           }"
   ],
   "id": "43cfa8df9dad01a0",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:41:17.785141Z",
     "start_time": "2024-10-01T04:41:17.776709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text)"
   ],
   "id": "73318d2ef12a3b2c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:41:19.206507Z",
     "start_time": "2024-10-01T04:41:19.202035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_emoji(string):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', string)"
   ],
   "id": "e57c9e8a1f353cff",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:41:22.102855Z",
     "start_time": "2024-10-01T04:41:22.097813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_punctuations(data):\n",
    "    punct_tag=re.compile(r'[^\\w\\s]')\n",
    "    data=punct_tag.sub(r'',data)\n",
    "    return data"
   ],
   "id": "d81286ca0027900",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:41:24.188055Z",
     "start_time": "2024-10-01T04:41:24.182736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def removeSpecialChars(data):\n",
    "    '''\n",
    "    Removes special characters which are specifically found in tweets.\n",
    "    '''\n",
    "    #Converts HTML tags to the characters they represent\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "    data = soup.get_text()\n",
    "\n",
    "    #Convert www.* or https?://* to empty strings\n",
    "    data = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',data)\n",
    "\n",
    "    #Convert @username to empty strings\n",
    "    data = re.sub('@[^\\s]+','',data)\n",
    "    \n",
    "    #remove org.apache. like texts\n",
    "    data =  re.sub('(\\w+\\.){2,}','',data)\n",
    "\n",
    "    #Remove additional white spaces\n",
    "    data = re.sub('[\\s]+', ' ', data)\n",
    "    \n",
    "    data = re.sub('\\.(?!$)', '', data)\n",
    "\n",
    "    #Replace #word with word\n",
    "    data = re.sub(r'#([^\\s]+)', r'\\1', data)\n",
    "\n",
    "    return data "
   ],
   "id": "c3ebf7d61fb562a4",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:41:32.047727Z",
     "start_time": "2024-10-01T04:41:32.043523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_nonenglish_charac(string):\n",
    "    return re.sub('\\W+','', string )"
   ],
   "id": "e8c4da400c471e5f",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:41:49.313990Z",
     "start_time": "2024-10-01T04:41:49.308862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords"
   ],
   "id": "915a6760215f6fb2",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:41:52.398471Z",
     "start_time": "2024-10-01T04:41:52.392741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "extra_punctuations = ['','.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', 'â€“','&']\n",
    "stopword_list = stopwords.words('english') + list(string.punctuation)+ extra_punctuations + ['u','the','us','say','that','he','me','she','get','rt','it','mt','via','not','and','let','so','say','dont','use','you']"
   ],
   "id": "94e245d9c0a6d2ab",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:41:54.518487Z",
     "start_time": "2024-10-01T04:41:54.512512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text(data):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer() \n",
    "    tokenizer=TweetTokenizer()\n",
    "    data = unidecode(data)\n",
    "    data = expandContractions(data)\n",
    "    tokens = tokenizer.tokenize(data)\n",
    "    data = ' '.join([tok for tok in tokens if len(tok) > 2 if tok not in stopword_list and not tok.isdigit()])\n",
    "    data = re.sub('\\b\\w{,2}\\b', '', data)\n",
    "    data = re.sub(' +', ' ', data)\n",
    "    data = removeSpecialChars(data)\n",
    "    data = remove_emoji(data)\n",
    "    data= [stemmer.stem(w) for w in data.split()]\n",
    "    return ' '.join([wordnet_lemmatizer.lemmatize(word) for word in data])"
   ],
   "id": "3881d89e595d7e28",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:43:37.988206Z",
     "start_time": "2024-10-01T04:43:37.980753Z"
    }
   },
   "cell_type": "code",
   "source": "import unidecode",
   "id": "99a0cbb9cfb3fcca",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:43:53.751702Z",
     "start_time": "2024-10-01T04:43:53.748567Z"
    }
   },
   "cell_type": "code",
   "source": "# train_df['text'] = train_df['text'].apply(lambda x: clean_text(x))",
   "id": "5b9ac627146f7f1f",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:43:51.588133Z",
     "start_time": "2024-10-01T04:43:51.562098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_y=train_df['label']\n",
    "train_x,val_x,train_y,val_y=train_test_split(train_df['text'],train_y,test_size=0.2,random_state=50)"
   ],
   "id": "2c73844d149b3e2f",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:43:57.552709Z",
     "start_time": "2024-10-01T04:43:57.547876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:    \n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    print('TPU is not initialized ')\n",
    "    tpu = None"
   ],
   "id": "c7cd730e2f3ddc4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPU is not initialized \n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:44:00.097087Z",
     "start_time": "2024-10-01T04:44:00.079062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "print(\"Replicas num: \", strategy.num_replicas_in_sync)\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "# Configuration of hyperparameters\n",
    "EPOCHS = 3\n",
    "#batch size denotes the partitioning amongst the cluster replicas.\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "MAX_LEN = 192"
   ],
   "id": "5790aed1c4767671",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replicas num:  1\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:44:03.386789Z",
     "start_time": "2024-10-01T04:44:03.381869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Tokenize the data and separate them in chunks of 256 units\n",
    "\n",
    "maxlen=512\n",
    "chunk_size=256\n",
    "def quick_encode(texts,tokenizer, maxlen=maxlen):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        return_attention_mask=False,\n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen,\n",
    "        truncation=True,\n",
    "    )\n",
    "    return np.array(enc_di[\"input_ids\"])"
   ],
   "id": "ce985378cf6fc7ef",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:44:08.419241Z",
     "start_time": "2024-10-01T04:44:08.413251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def built_model(transformer,train_data,val_data,batch_size,img_name, max_len=512):\n",
    "    inp_words_ids = Input(shape =(max_len,),dtype = tf.int32,name=\"input_word_ids\")\n",
    "    seq_output = transformer(inp_words_ids)[0]\n",
    "    cls_token = seq_output[:,0,:]\n",
    "    output =  Dense(3,activation='softmax')(cls_token)\n",
    "    model = Model(inputs =inp_words_ids,outputs=output)\n",
    "    model.compile(Adam(lr=1e-5),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    plot_model(\n",
    "        model,to_file=img_name,\n",
    "        show_shapes=True,\n",
    "        show_layer_names=True,\n",
    "        rankdir=\"TB\",\n",
    "        expand_nested=False,\n",
    "        dpi=96)\n",
    "    model.fit(\n",
    "        train_data,\n",
    "        steps_per_epoch=batch_size,\n",
    "        validation_data=val_data,\n",
    "        epochs=EPOCHS\n",
    "        )"
   ],
   "id": "4095394f80e8c7e5",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:44:11.700996Z",
     "start_time": "2024-10-01T04:44:11.696995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def del_obects(*args):\n",
    "    for arg in args:\n",
    "        del arg\n",
    "        gc.collect()"
   ],
   "id": "101c14c51da9709b",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:45:24.806480Z",
     "start_time": "2024-10-01T04:45:21.607501Z"
    }
   },
   "cell_type": "code",
   "source": "import transformers",
   "id": "434d70d9211cc1f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YC\\.conda\\envs\\SDPtest\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T04:59:22.307813Z",
     "start_time": "2024-10-01T04:59:22.283641Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import TFAutoModel",
   "id": "c275ed261623a958",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:03:27.463005Z",
     "start_time": "2024-10-01T04:59:24.444413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with strategy.scope():\n",
    "\n",
    "    distilbert_tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "    train_x_enc= quick_encode(train_x.astype(str),distilbert_tokenizer, maxlen=MAX_LEN)\n",
    "    val_x_enc = quick_encode(val_x.astype(str),distilbert_tokenizer,maxlen=MAX_LEN)\n",
    "    train_dataset = (\n",
    "                    tf.data.Dataset.from_tensor_slices((train_x_enc,train_y))\n",
    "                    .repeat()\n",
    "                    .shuffle(2048)\n",
    "                    .batch(BATCH_SIZE)\n",
    "                    .prefetch(AUTO)\n",
    "                    )\n",
    "    valid_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices((val_x_enc,val_y))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .cache()\n",
    "        .prefetch(AUTO)                \n",
    "    )\n",
    "    transformer_layer = TFAutoModel.from_pretrained('distilbert-base-multilingual-cased')\n",
    "    built_model(transformer_layer,train_dataset,valid_dataset,train_x_enc.shape[0],\n",
    "                \"Distilbert_Multilingual_Transformer.png\",max_len=MAX_LEN)\n",
    "    \n",
    "    del_obects(train_x_enc,val_x_enc,train_dataset,valid_dataset,distilbert_tokenizer,transformer_layer)"
   ],
   "id": "d2c65b1952e05f51",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YC\\.conda\\envs\\SDPtest\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\YC\\.conda\\envs\\SDPtest\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "C:\\Users\\YC\\.conda\\envs\\SDPtest\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\YC\\.conda\\envs\\SDPtest\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\YC\\.conda\\envs\\SDPtest\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[38], line 20\u001B[0m\n\u001B[0;32m     13\u001B[0m valid_dataset \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     14\u001B[0m     tf\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataset\u001B[38;5;241m.\u001B[39mfrom_tensor_slices((val_x_enc,val_y))\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;241m.\u001B[39mbatch(BATCH_SIZE)\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;241m.\u001B[39mcache()\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;241m.\u001B[39mprefetch(AUTO)                \n\u001B[0;32m     18\u001B[0m )\n\u001B[0;32m     19\u001B[0m transformer_layer \u001B[38;5;241m=\u001B[39m TFAutoModel\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdistilbert-base-multilingual-cased\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 20\u001B[0m built_model(transformer_layer,train_dataset,valid_dataset,train_x_enc\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m     21\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDistilbert_Multilingual_Transformer.png\u001B[39m\u001B[38;5;124m\"\u001B[39m,max_len\u001B[38;5;241m=\u001B[39mMAX_LEN)\n\u001B[0;32m     23\u001B[0m del_obects(train_x_enc,val_x_enc,train_dataset,valid_dataset,distilbert_tokenizer,transformer_layer)\n",
      "Cell \u001B[1;32mIn[31], line 2\u001B[0m, in \u001B[0;36mbuilt_model\u001B[1;34m(transformer, train_data, val_data, batch_size, img_name, max_len)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbuilt_model\u001B[39m(transformer,train_data,val_data,batch_size,img_name, max_len\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m):\n\u001B[1;32m----> 2\u001B[0m     inp_words_ids \u001B[38;5;241m=\u001B[39m Input(shape \u001B[38;5;241m=\u001B[39m(max_len,),dtype \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mint32,name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_word_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m     seq_output \u001B[38;5;241m=\u001B[39m transformer(inp_words_ids)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m      4\u001B[0m     cls_token \u001B[38;5;241m=\u001B[39m seq_output[:,\u001B[38;5;241m0\u001B[39m,:]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Input' is not defined"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:24:05.747882Z",
     "start_time": "2024-10-01T05:21:09.809742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with strategy.scope():\n",
    "\n",
    "    xlm_roberta_tokenizer = transformers.XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "    train_x_enc= quick_encode(train_x.astype(str),xlm_roberta_tokenizer, maxlen=MAX_LEN)\n",
    "    val_x_enc = quick_encode(val_x.astype(str),xlm_roberta_tokenizer,maxlen=MAX_LEN)\n",
    "    train_dataset = (\n",
    "                    tf.data.Dataset.from_tensor_slices((train_x_enc,train_y))\n",
    "                    .repeat()\n",
    "                    .shuffle(2048)\n",
    "                    .batch(BATCH_SIZE)\n",
    "                    .prefetch(AUTO)\n",
    "                    )\n",
    "    valid_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices((val_x_enc,val_y))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .cache()\n",
    "        .prefetch(AUTO)                \n",
    "    )\n",
    "    transformer_layer = TFAutoModel.from_pretrained('xlm-roberta-base')\n",
    "    built_model(transformer_layer,train_dataset,valid_dataset,train_x_enc.shape[0],\n",
    "                \"Roberta-Transformer.png\",max_len=MAX_LEN)\n",
    "    \n",
    "    del_obects(train_x_enc,val_x_enc,train_dataset,valid_dataset,xlm_roberta_tokenizer,transformer_layer)"
   ],
   "id": "74eeefcd2a684a8f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YC\\.conda\\envs\\SDPtest\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\YC\\.cache\\huggingface\\hub\\models--xlm-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\YC\\.conda\\envs\\SDPtest\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\YC\\.conda\\envs\\SDPtest\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "C:\\Users\\YC\\.conda\\envs\\SDPtest\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFXLMRobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing TFXLMRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFXLMRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[39], line 20\u001B[0m\n\u001B[0;32m     13\u001B[0m valid_dataset \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     14\u001B[0m     tf\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataset\u001B[38;5;241m.\u001B[39mfrom_tensor_slices((val_x_enc,val_y))\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;241m.\u001B[39mbatch(BATCH_SIZE)\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;241m.\u001B[39mcache()\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;241m.\u001B[39mprefetch(AUTO)                \n\u001B[0;32m     18\u001B[0m )\n\u001B[0;32m     19\u001B[0m transformer_layer \u001B[38;5;241m=\u001B[39m TFAutoModel\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxlm-roberta-base\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 20\u001B[0m built_model(transformer_layer,train_dataset,valid_dataset,train_x_enc\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m     21\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRoberta-Transformer.png\u001B[39m\u001B[38;5;124m\"\u001B[39m,max_len\u001B[38;5;241m=\u001B[39mMAX_LEN)\n\u001B[0;32m     23\u001B[0m del_obects(train_x_enc,val_x_enc,train_dataset,valid_dataset,xlm_roberta_tokenizer,transformer_layer)\n",
      "Cell \u001B[1;32mIn[31], line 2\u001B[0m, in \u001B[0;36mbuilt_model\u001B[1;34m(transformer, train_data, val_data, batch_size, img_name, max_len)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbuilt_model\u001B[39m(transformer,train_data,val_data,batch_size,img_name, max_len\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m):\n\u001B[1;32m----> 2\u001B[0m     inp_words_ids \u001B[38;5;241m=\u001B[39m Input(shape \u001B[38;5;241m=\u001B[39m(max_len,),dtype \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mint32,name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_word_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m     seq_output \u001B[38;5;241m=\u001B[39m transformer(inp_words_ids)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m      4\u001B[0m     cls_token \u001B[38;5;241m=\u001B[39m seq_output[:,\u001B[38;5;241m0\u001B[39m,:]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Input' is not defined"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with strategy.scope():\n",
    "\n",
    "    gpt2_tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "    train_x_enc= quick_encode(train_x.astype(str),gpt2_tokenizer, maxlen=MAX_LEN)\n",
    "    val_x_enc = quick_encode(val_x.astype(str),gpt2_tokenizer,maxlen=MAX_LEN)\n",
    "    train_dataset = (\n",
    "                    tf.data.Dataset.from_tensor_slices((train_x_enc,train_y))\n",
    "                    .repeat()\n",
    "                    .shuffle(2048)\n",
    "                    .batch(BATCH_SIZE)\n",
    "                    .prefetch(AUTO)\n",
    "                    )\n",
    "    valid_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices((val_x_enc,val_y))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .cache()\n",
    "        .prefetch(AUTO)                \n",
    "    )\n",
    "    transformer_layer = TFAutoModel.from_pretrained('gpt2-medium')\n",
    "    built_model(transformer_layer,train_dataset,valid_dataset,train_x_enc.shape[0],\n",
    "                \"GPT2-Transformer.png\",max_len=MAX_LEN)\n",
    "    \n",
    "    del_obects(train_x_enc,val_x_enc,train_dataset,valid_dataset,gpt2_tokenizer,transformer_layer"
   ],
   "id": "ad60f5d4bca4b5f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e76dcebd066cde9d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
