{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T07:04:42.999620Z",
     "start_time": "2024-08-20T07:04:42.992989Z"
    }
   },
   "source": [
    "import string\n",
    "import scipy\n",
    "import nltk\n",
    "from nltk import PorterStemmer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from ordered_set import OrderedSet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "638a95c5-782f-43c5-b87b-01ce1c838cef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T07:01:43.694510Z",
     "start_time": "2024-08-20T07:01:43.673451Z"
    }
   },
   "source": [
    "def get_and_clean_data():\n",
    "    data = pd.read_excel('resources/JDT.xlsx')\n",
    "    description = data['summary']\n",
    "    cleaned_description = description.apply(lambda s: s.translate(str.maketrans('', '', string.punctuation + u'\\xa0')))\n",
    "    cleaned_description = cleaned_description.apply(lambda s: s.lower())\n",
    "    cleaned_description = cleaned_description.apply(lambda s: s.translate(str.maketrans(string.whitespace, ' '*len(string.whitespace), '')))\n",
    "    cleaned_description = cleaned_description.drop_duplicates()\n",
    "    return cleaned_description"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "4b639049c281f955",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T07:01:52.242479Z",
     "start_time": "2024-08-20T07:01:52.232675Z"
    }
   },
   "source": [
    "def create_stem_cache(cleaned_description):\n",
    "    tokenized_description = cleaned_description.apply(lambda s: word_tokenize(s))\n",
    "    concated = np.unique(np.concatenate([s for s in tokenized_description.values]))\n",
    "    stem_cache = {}\n",
    "    ps = PorterStemmer()\n",
    "    for s in concated:\n",
    "        stem_cache[s] = ps.stem(s)\n",
    "    return stem_cache"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "578c42a1f6e42993",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T07:01:56.795156Z",
     "start_time": "2024-08-20T07:01:56.786458Z"
    }
   },
   "source": [
    "def create_custom_preprocessor(stop_dict, stem_cache):\n",
    "    def custom_preprocessor(s):\n",
    "        ps = PorterStemmer()\n",
    "        s = re.sub(r'[^A-Za-z]', ' ', s)\n",
    "        s = re.sub(r'\\s+', ' ', s)\n",
    "        s = word_tokenize(s)\n",
    "        s = list(OrderedSet(s)- stop_dict)\n",
    "        s = [word for word in s if len(word)>2]\n",
    "        s = [stem_cache[w] if w in stem_cache else ps.stem(w) for w in s]\n",
    "        s = ' '.join(s)\n",
    "        return s\n",
    "    return custom_preprocessor"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "ea0eb280355aa727",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T07:02:03.323062Z",
     "start_time": "2024-08-20T07:02:03.316108Z"
    }
   },
   "source": [
    "def sk_vectorize(texts, cleaned_description, stop_dict, stem_cache):\n",
    "    my_custom_preprocessor = create_custom_preprocessor(stop_dict, stem_cache)\n",
    "    vectorizer = CountVectorizer(preprocessor=my_custom_preprocessor)\n",
    "    vectorizer.fit(cleaned_description)\n",
    "    query = vectorizer.transform(texts)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "e3d88302-998b-4600-b5fe-f3f7adbf0db7",
   "metadata": {},
   "source": [
    "# tfidf"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T14:34:03.566230Z",
     "start_time": "2024-08-26T14:34:01.528527Z"
    }
   },
   "cell_type": "code",
   "source": "import nltk",
   "id": "47287cd13522cb5b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "b546b24e-6bbc-42c5-a6b2-78b3ce5303e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T14:34:06.701839Z",
     "start_time": "2024-08-26T14:34:06.042015Z"
    }
   },
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "cleaned_description = get_and_clean_data()\n",
    "stem_cache = create_stem_cache(cleaned_description)\n",
    "stop_dict = set(stopwords.words('English'))\n",
    "my_custom_preprocessor = create_custom_preprocessor(stop_dict, stem_cache)\n",
    "vectorizer_unigram = CountVectorizer(preprocessor=my_custom_preprocessor)\n",
    "vectorizer_unigram.fit(cleaned_description)\n",
    "X_unigram = vectorizer_unigram.transform(cleaned_description)\n",
    "N_unigram = len(cleaned_description)\n",
    "df_unigram = np.array((X_unigram.todense() > 0).sum(0))[0]\n",
    "idf_unigram = np.log10(1 + (N_unigram / df_unigram))\n",
    "tf_unigram = np.log10(X_unigram.todense() + 1)\n",
    "tf_idf_unigram = np.multiply(tf_unigram, idf_unigram)\n",
    "X_unigram = scipy.sparse.csr_matrix(tf_idf_unigram)\n",
    "X_df_unigram = pd.DataFrame(X_unigram.toarray(), columns=vectorizer_unigram.get_feature_names_out())\n",
    "max_term_unigram = X_df_unigram.sum().sort_values(ascending=False)[:20].index"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\YC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\YC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_and_clean_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m nltk\u001B[38;5;241m.\u001B[39mdownload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpunkt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      2\u001B[0m nltk\u001B[38;5;241m.\u001B[39mdownload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstopwords\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m cleaned_description \u001B[38;5;241m=\u001B[39m get_and_clean_data()\n\u001B[0;32m      4\u001B[0m stem_cache \u001B[38;5;241m=\u001B[39m create_stem_cache(cleaned_description)\n\u001B[0;32m      5\u001B[0m stop_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(stopwords\u001B[38;5;241m.\u001B[39mwords(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEnglish\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'get_and_clean_data' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BM25",
   "id": "a36a8ff3aabfb5d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a3b74-fcae-46f3-9a97-f810772de0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25(object):\n",
    "    def __init__(self, vectorizer, b=0.75, k1=1.6):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.b = b\n",
    "        self.k1 = k1\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\" Fit IDF to documents X \"\"\"\n",
    "        self.vectorizer.fit(X)\n",
    "        self.y = super(TfidfVectorizer, self.vectorizer).transform(X)\n",
    "        self.avdl = self.y.sum(1).mean()\n",
    "\n",
    "    def transform(self, q):\n",
    "        \"\"\" Calculate BM25 between query q and documents X \"\"\"\n",
    "        b, k1, avdl = self.b, self.k1, self.avdl\n",
    "\n",
    "        # apply CountVectorizer\n",
    "        len_y = self.y.sum(1).A1\n",
    "        q, = super(TfidfVectorizer, self.vectorizer).transform([q])\n",
    "        assert sparse.isspmatrix_csr(q)\n",
    "\n",
    "        y = self.y.tocsc()[:, q.indices]\n",
    "        denom = y + (k1 * (1 - b + b * len_y / avdl))[:, None]\n",
    "        idf = self.vectorizer._tfidf.idf_[None, q.indices] - 1.\n",
    "        numer = y.multiply(np.broadcast_to(idf, y.shape)) * (k1 + 1)\n",
    "        return (numer / denom).sum(1).A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223e1079-bb16-4fd4-bf45-c550938d2207",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_description = get_and_clean_data()\n",
    "stem_cache = create_stem_cache(cleaned_description)\n",
    "stop_dict = set(stopwords.words('English'))\n",
    "my_custom_preprocessor = create_custom_preprocessor(stop_dict, stem_cache)\n",
    "tf_idf_vectorizer = TfidfVectorizer(preprocessor=my_custom_preprocessor, use_idf=True)\n",
    "tf_idf_vectorizer.fit(cleaned_description)\n",
    "bm25 = BM25(tf_idf_vectorizer)\n",
    "bm25.fit(cleaned_description)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "tf-idf+lsd+lda",
   "id": "98788ca3f91d9ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc4ac3-2daf-4632-864f-0474a679bfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a61cb-afda-4957-9d3f-0fd3764a630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1,1)) \n",
    "count_vectorizer.fit(cleaned_title + cleaned_body) \n",
    "X_tf_fit = count_vectorizer.transform(data_fit['title']) \n",
    "X_tf_blindtest = count_vectorizer.transform(data_blindtest['title']) \n",
    "lda = LatentDirichletAllocation(n_components=500, random_state=0) \n",
    "lda.fit(X_tf_fit) \n",
    "X_lda_fit = lda.transform(X_tf_fit)\n",
    "gbm_model_with_lda = lgb.LGBMClassifier() \n",
    "\n",
    "precision_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_lda_fit, y_fit, cv=5, n_jobs=-2, scoring='precision_macro').mean() \n",
    "recall_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_lda_fit, y_fit, cv=5, n_jobs=-2, scoring='recall_macro').mean() \n",
    "f1_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_lda_fit, y_fit, cv=5, n_jobs=-2, scoring='f1_macro').mean() \n",
    "\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score)) \n",
    "\n",
    "X_fit_with_lda = hstack([X_tfidf_fit, X_lda_fit]).tocsr() \n",
    "\n",
    "precision_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_fit_with_lda, y_fit, cv=5, n_jobs=-2, scoring='precision_macro').mean() \n",
    "recall_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_fit_with_lda, y_fit, cv=5, n_jobs=-2, scoring='recall_macro').mean() \n",
    "f1_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_fit_with_lda, y_fit, cv=5, \n",
    "n_jobs=-2, scoring='f1_macro').mean() \n",
    "\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c49b573-80ca-48a3-ad66-cd0dcbf91b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_model_with_lsa_lda = lgb.LGBMClassifier()\n",
    "X_fit_with_lsa_lda = hstack([X_tfidf_fit, X_lsa_fit,X_lda_fit]).tocsr()\n",
    "precision_cv_score = model_selection.cross_val_score(gbm_model_with_lsa_lda, X_fit_with_lsa_lda, y_fit, cv=5, n_jobs=-2, scoring='precision_macro').mean() \n",
    "recall_cv_score = model_selection.cross_val_score(gbm_model_with_lsa_lda, X_fit_with_lsa_lda, y_fit, cv=5, n_jobs=-2, scoring='recall_macro').mean() \n",
    "f1_cv_score = model_selection.cross_val_score(gbm_model_with_lsa_lda, X_fit_with_lsa_lda, y_fit, cv=5, \n",
    "n_jobs=-2, scoring='f1_macro').mean() \n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "tfidf+lsa 降维处理",
   "id": "21c75bab28cf07fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249603c6-cd12-475c-b553-82b76c8f9655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import hstack\n",
    "lsa = TruncatedSVD(n_components=500, n_iter=100, random_state=0)\n",
    "lsa.fit(X_tfidf_fit)\n",
    "X_lsa_fit = lsa.transform(X_tfidf_fit)\n",
    "\n",
    "gbm_model_with_lsa = lgb.LGBMClassifier()\n",
    "\n",
    "precision_cv_score = model_selection.cross_val_score(gbm_model_with_lsa, X_lsa_fit, y_fit, cv=5, n_jobs=-2, scoring='precision_macro').mean()\n",
    "recall_cv_score = model_selection.cross_val_score(gbm_model_with_lsa, X_lsa_fit, y_fit, cv=5, n_jobs=-2, scoring='recall_macro').mean()\n",
    "f1_cv_score = model_selection.cross_val_score(gbm_model_with_lsa, X_lsa_fit, y_fit, cv=5, n_jobs=-2, scoring='f1_macro').mean()\n",
    "\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n",
    "\n",
    "X_fit_with_lsa = hstack([X_tfidf_fit, X_lsa_fit]).tocsr()\n",
    "\n",
    "precision_cv_score = model_selection.cross_val_score(gbm_model_with_lsa, X_fit_with_lsa, y_fit, cv=5, n_jobs=-2, scoring='precision_macro').mean()\n",
    "recall_cv_score = model_selection.cross_val_score(gbm_model_with_lsa, X_fit_with_lsa, y_fit, cv=5, n_jobs=-2, scoring='recall_macro').mean()\n",
    "f1_cv_score = model_selection.cross_val_score(gbm_model_with_lsa, X_fit_with_lsa, y_fit, cv=5, n_jobs=-2, scoring='f1_macro').mean()\n",
    "\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06690f48-a32e-424b-b3d1-467ab3a9bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf+LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358905a5-e149-487e-ab1b-6e582eac9396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aed4b7-8f07-443e-8771-8cf7e526772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1,1)) \n",
    "count_vectorizer.fit(cleaned_title + cleaned_body) \n",
    "X_tf_fit = count_vectorizer.transform(data_fit['title']) \n",
    "# X_tf_blindtest = count_vectorizer.transform(data_blindtest['title']) \n",
    "lda = LatentDirichletAllocation(n_components=500, random_state=0) \n",
    "lda.fit(X_tf_fit) \n",
    "X_lda_fit = lda.transform(X_tf_fit)\n",
    "gbm_model_with_lda = gbm_model\n",
    "\n",
    "X_fit_with_lda = hstack([X_tfidf_fit, X_lda_fit]).tocsr() \n",
    "\n",
    "precision_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_fit_with_lda, y_fit, cv=5, n_jobs=-2, scoring='precision_macro').mean() \n",
    "recall_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_fit_with_lda, y_fit, cv=5, n_jobs=-2, scoring='recall_macro').mean() \n",
    "f1_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_fit_with_lda, y_fit, cv=5, \n",
    "n_jobs=-2, scoring='f1_macro').mean() \n",
    "\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f897de-5031-4024-abef-66fab59c55e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e10036-5260-4aed-99b0-076f13df358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to perform dimension reduction and build the model\n",
    "# 只是为了降维和建构模型\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# TruncatedSVD 初始化：创建了一个TruncatedSVD（截断奇异值分解）的实例 lsa，设置了 n_components=500 表示要降维到的目标维度为500。\n",
    "lsa = TruncatedSVD(n_components=500, n_iter=100, random_state=0)\n",
    "# 拟合 TruncatedSVD：使用训练集的 TF-IDF 特征 X_tfidf_fit 对 TruncatedSVD 模型进行拟合。\n",
    "lsa.fit(X_tfidf_fit)\n",
    "# TruncatedSVD 转换：使用拟合好的 TruncatedSVD 模型将训练集的 TF-IDF 特征降维为500维。\n",
    "X_lsa_fit = lsa.transform(X_tfidf_fit)\n",
    "\n",
    "# LGBM 模型初始化：创建了一个新的LightGBM分类器实例 gbm_model_with_lsa。\n",
    "gbm_model_with_lsa = lgb.LGBMClassifier()\n",
    "\n",
    "# 交叉验证：使用交叉验证对 LightGBM 模型进行评估，分别计算了精确率、召回率和 F1 分数。\n",
    "precision_cv_score = model_selection.cross_val_score(gbm_model_with_lsa, X_lsa_fit, y_fit, cv=5, n_jobs=-2, scoring='precision_macro').mean()\n",
    "recall_cv_score = model_selection.cross_val_score(gbm_model_with_lsa, X_lsa_fit, y_fit, cv=5, n_jobs=-2, scoring='recall_macro').mean()\n",
    "f1_cv_score = model_selection.cross_val_score(gbm_model_with_lsa, X_lsa_fit, y_fit, cv=5, n_jobs=-2, scoring='f1_macro').mean()\n",
    "\n",
    "# 打印结果\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n",
    "\n",
    "# 结合 TF-IDF 和 TruncatedSVD 特征：将 TF-IDF 特征和降维后的 TruncatedSVD 特征水平堆叠，以创建新的特征矩阵。\n",
    "X_fit_with_lsa = hstack([X_tfidf_fit, X_lsa_fit]).tocsr()\n",
    "\n",
    "# 再次交叉验证：使用新的特征矩阵对 LightGBM 模型进行再次交叉验证，计算精确率、召回率和 F1 分数。\n",
    "precision_cv_score = model_selection.cross_val_score(gbm_model_with_lsa, X_fit_with_lsa, y_fit, cv=5, n_jobs=-2, scoring='precision_macro').mean()\n",
    "recall_cv_score = model_selection.cross_val_score(gbm_model_with_lsa, X_fit_with_lsa, y_fit, cv=5, n_jobs=-2, scoring='recall_macro').mean()\n",
    "f1_cv_score = model_selection.cross_val_score(gbm_model_with_lsa, X_fit_with_lsa, y_fit, cv=5, n_jobs=-2, scoring='f1_macro').mean()\n",
    "\n",
    "# 打印结果：将交叉验证得到的精确率、召回率和 F1 分数打印出来。\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65bce32-97e6-4dbb-93a2-470c53a52ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea542b3-3a8d-48a7-8073-3187e6fd765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an LDA model as a preprocessor\n",
    "# 拟合LDA模型作为预处理器\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# CountVectorizer 初始化：创建一个 CountVectorizer 实例 count_vectorizer，用于将文本转换为词频向量。\n",
    "# 设置 ngram_range=(1,1) 表示只考虑单个词（unigram）的词频。\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "# 拟合 CountVectorizer：使用训练集的标题和正文数据拟合 CountVectorizer。\n",
    "count_vectorizer.fit(cleaned_title + cleaned_body)\n",
    "# 转换数据集：使用 CountVectorizer 将训练集和测试集的标题数据转换为词频向量。\n",
    "X_tf_fit = count_vectorizer.transform(data_fit['title'])\n",
    "X_tf_blindtest = count_vectorizer.transform(data_blindtest['title'])\n",
    "# LDA 拟合：使用拟合好的词频向量 X_tf_fit 对 LDA 模型进行拟合，设置了 n_components=500 表示要学习的主题数为 500。\n",
    "lda = LatentDirichletAllocation(n_components=500, random_state=0)\n",
    "lda.fit(X_tf_fit)\n",
    "# LDA 转换：使用拟合好的 LDA 模型将训练集的词频向量转换为 LDA 主题向量。\n",
    "X_lda_fit = lda.transform(X_tf_fit)\n",
    "# LGBM 模型初始化：创建一个 LightGBM 分类器实例 gbm_model_with_lda。\n",
    "gbm_model_with_lda = lgb.LGBMClassifier()\n",
    "\n",
    "# 交叉验证：使用交叉验证对 LightGBM 模型进行评估，分别计算了精确率、召回率和 F1 分数。\n",
    "precision_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_lda_fit, y_fit, cv=5, n_jobs=-2, scoring='precision_macro').mean()\n",
    "recall_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_lda_fit, y_fit, cv=5, n_jobs=-2, scoring='recall_macro').mean() \n",
    "f1_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_lda_fit, y_fit, cv=5, n_jobs=-2, scoring='f1_macro').mean()\n",
    "# 打印结果\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n",
    "\n",
    "# 结合 TF-IDF 和 LDA 特征：将 TF-IDF 特征和 LDA 主题向量水平堆叠，以创建新的特征矩阵。\n",
    "X_fit_with_lda = hstack([X_tfidf_fit, X_lda_fit]).tocsr()\n",
    "\n",
    "# 再次交叉验证：使用新的特征矩阵对 LightGBM 模型进行再次交叉验证，计算精确率、召回率和 F1 分数。\n",
    "precision_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_fit_with_lda, y_fit, cv=5, n_jobs=-2, scoring='precision_macro').mean()\n",
    "recall_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_fit_with_lda, y_fit, cv=5, n_jobs=-2, scoring='recall_macro').mean()\n",
    "f1_cv_score = model_selection.cross_val_score(gbm_model_with_lda, X_fit_with_lda, y_fit, cv=5, n_jobs=-2, scoring='f1_macro').mean()\n",
    "# 打印结果\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
