{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-01T05:27:24.550825Z",
     "start_time": "2024-10-01T05:27:23.022946Z"
    }
   },
   "source": [
    "# fix for autocomplete intellisense\n",
    "#%config Completer.use_jedi = False\n",
    "\n",
    "#for printing all the outputs of a cell in the same output window\n",
    "from IPython.core.interactiveshell import InteractiveShell  \n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#InteractiveShell.ast_node_interactivity = \"last_expr\"\n",
    "\n",
    "\n",
    "# Basic Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "import itertools\n",
    "import collections\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "from sklearn import utils\n",
    "\n",
    "\n",
    "# Visualization\n",
    "\n",
    "# matplotlib\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# seaborn\n",
    "import seaborn as sns\n",
    "sns.set_style(style='whitegrid')\n",
    "\n",
    "# plotly\n",
    "!pip3 install chart_studio\n",
    "import chart_studio.plotly as py\n",
    "\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "\n",
    "# cufflinks\n",
    "!pip3 install cufflinks\n",
    "import cufflinks\n",
    "cufflinks.go_offline()\n",
    "cufflinks.set_config_file(world_readable=True, theme='pearl')\n",
    "\n",
    "# nltk wordcloud\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "\n",
    "# dimensionality reduction\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, SparsePCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "!pip3 install beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nlp_spcy = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\",\"ner\"])\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "STOP_WORDS = list(set(STOP_WORDS))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,  cross_val_predict\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV\n",
    "\n",
    "# from imblearn.over_sampling import ADASYN, SMOTE\n",
    "# from imblearn.under_sampling import NearMiss\n",
    "\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier as xgb\n",
    "from lightgbm import LGBMClassifier as lgbm\n",
    "\n",
    "\n",
    "# Gensim models\n",
    "from smart_open import open\n",
    "from gensim.models import Word2Vec, FastText, KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "#from gensim.models.fasttext import load_facebook_vectors\n",
    "#from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import  accuracy_score, log_loss, classification_report\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "\n",
    "# model serialization\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"ignore\")\n",
    "\n",
    "\n",
    "# Settings for pretty nice plots\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## for deep learning\n",
    "\n",
    "##% tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import models, layers, preprocessing as KP\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import backend as KB\n",
    "\n",
    "## for bert language model\n",
    "# import transformers"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_profiling'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 14\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas_profiling\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mstring\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'pandas_profiling'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:28:05.471131Z",
     "start_time": "2024-10-01T05:28:04.427620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = pd.read_json('resources/embold_train.json')\n",
    "print(\"Train Dataframe:\")\n",
    "train_df.head(3)\n",
    "print(f'Train dataframe contains {train_df.shape[0]} samples.')\n",
    "print('Number of features in train data : ', train_df.shape[1])\n",
    "print('Train Features : ', train_df.columns.values)"
   ],
   "id": "a797b97ad756a639",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataframe:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                         title  \\\n",
       "0            y-zoom piano roll   \n",
       "1  buggy behavior in selection   \n",
       "2          auto update feature   \n",
       "\n",
       "                                                body  label  \n",
       "0        a y-zoom on the piano roll would be useful.      1  \n",
       "1  ! screenshot from 2016-02-23 21 27 40  https:/...      0  \n",
       "2  hi,\\r \\r great job so far, @saenzramiro ! : \\r...      1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y-zoom piano roll</td>\n",
       "      <td>a y-zoom on the piano roll would be useful.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buggy behavior in selection</td>\n",
       "      <td>! screenshot from 2016-02-23 21 27 40  https:/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auto update feature</td>\n",
       "      <td>hi,\\r \\r great job so far, @saenzramiro ! : \\r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataframe contains 150000 samples.\n",
      "Number of features in train data :  3\n",
      "Train Features :  ['title' 'body' 'label']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:28:33.685492Z",
     "start_time": "2024-10-01T05:28:31.253090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_extra_df = pd.read_json('resources/embold_train_extra.json')\n",
    "train_extra_df.head()\n",
    "print('')\n",
    "print(f'Extra train dataset contains {train_extra_df.shape[0]} samples.')\n",
    "print('Number of features in the extra train dataset : ', train_extra_df.shape[1])\n",
    "print('Features in the extra train dataset : ', train_extra_df.columns.values)"
   ],
   "id": "8b4564ad8be3f898",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               title  \\\n",
       "0                                use a 8bit typeface   \n",
       "1                   implement wireless m-bus binding   \n",
       "2               add multilang support for timeago.js   \n",
       "3                   scaleway - seg-fault on shutdown   \n",
       "4  sistema de pintura: no se guardar los nuevos p...   \n",
       "\n",
       "                                                body  label  \n",
       "0  since this is meant to emulate some old arcade...      1  \n",
       "1  _from  chris.pa...@googlemail.com  https://cod...      1  \n",
       "2  currently it is only  en . \\r required to add ...      1  \n",
       "3  tbr  irc  creates a new scaleway instance with...      0  \n",
       "4  este sp ya estaba asignado a un carro y se enc...      0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>use a 8bit typeface</td>\n",
       "      <td>since this is meant to emulate some old arcade...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>implement wireless m-bus binding</td>\n",
       "      <td>_from  chris.pa...@googlemail.com  https://cod...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>add multilang support for timeago.js</td>\n",
       "      <td>currently it is only  en . \\r required to add ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scaleway - seg-fault on shutdown</td>\n",
       "      <td>tbr  irc  creates a new scaleway instance with...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sistema de pintura: no se guardar los nuevos p...</td>\n",
       "      <td>este sp ya estaba asignado a un carro y se enc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extra train dataset contains 300000 samples.\n",
      "Number of features in the extra train dataset :  3\n",
      "Features in the extra train dataset :  ['title' 'body' 'label']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:28:42.334822Z",
     "start_time": "2024-10-01T05:28:41.317509Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.profile_report()",
   "id": "693f9f1edc20b3b6",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'profile_report'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_df\u001B[38;5;241m.\u001B[39mprofile_report()\n",
      "File \u001B[1;32m~\\.conda\\envs\\SDPtest\\Lib\\site-packages\\pandas\\core\\generic.py:6299\u001B[0m, in \u001B[0;36mNDFrame.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   6292\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   6293\u001B[0m     name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_internal_names_set\n\u001B[0;32m   6294\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_metadata\n\u001B[0;32m   6295\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_accessors\n\u001B[0;32m   6296\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info_axis\u001B[38;5;241m.\u001B[39m_can_hold_identifiers_and_holds_name(name)\n\u001B[0;32m   6297\u001B[0m ):\n\u001B[0;32m   6298\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m[name]\n\u001B[1;32m-> 6299\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mobject\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getattribute__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'profile_report'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:28:56.919546Z",
     "start_time": "2024-10-01T05:28:56.861403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Null values and Data types\n",
    "print('Train Set:\\n')\n",
    "print(train_df.info())\n",
    "print('')"
   ],
   "id": "9cb77001cd1fa75f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150000 entries, 0 to 149999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   title   150000 non-null  object\n",
      " 1   body    150000 non-null  object\n",
      " 2   label   150000 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 3.4+ MB\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:29:07.795765Z",
     "start_time": "2024-10-01T05:29:07.762519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check the data for null values\n",
    "print('Train data Null values :')\n",
    "train_df.isnull().sum()"
   ],
   "id": "a61271ec4cc4871",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data Null values :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "title    0\n",
       "body     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:29:14.586510Z",
     "start_time": "2024-10-01T05:29:14.282792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check the basic stats\n",
    "print('Train set basic stats:')\n",
    "train_df.describe(include='all')"
   ],
   "id": "a7be36837b82886b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set basic stats:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                 title                                         body  \\\n",
       "count           150000                                       150000   \n",
       "unique          149677                                       150000   \n",
       "top     add unit tests  a y-zoom on the piano roll would be useful.   \n",
       "freq                15                                            1   \n",
       "mean               NaN                                          NaN   \n",
       "std                NaN                                          NaN   \n",
       "min                NaN                                          NaN   \n",
       "25%                NaN                                          NaN   \n",
       "50%                NaN                                          NaN   \n",
       "75%                NaN                                          NaN   \n",
       "max                NaN                                          NaN   \n",
       "\n",
       "                label  \n",
       "count   150000.000000  \n",
       "unique            NaN  \n",
       "top               NaN  \n",
       "freq              NaN  \n",
       "mean         0.648267  \n",
       "std          0.644655  \n",
       "min          0.000000  \n",
       "25%          0.000000  \n",
       "50%          1.000000  \n",
       "75%          1.000000  \n",
       "max          2.000000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150000</td>\n",
       "      <td>150000</td>\n",
       "      <td>150000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>149677</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>add unit tests</td>\n",
       "      <td>a y-zoom on the piano roll would be useful.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.648267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.644655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:29:22.355446Z",
     "start_time": "2024-10-01T05:29:22.244488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df['text'] = train_df.title + ' ' + train_df.body\n",
    "print(\"Train Dataframe with combined title and body text:\")\n",
    "train_df.head(3)"
   ],
   "id": "2066f47ad48d25c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataframe with combined title and body text:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                         title  \\\n",
       "0            y-zoom piano roll   \n",
       "1  buggy behavior in selection   \n",
       "2          auto update feature   \n",
       "\n",
       "                                                body  label  \\\n",
       "0        a y-zoom on the piano roll would be useful.      1   \n",
       "1  ! screenshot from 2016-02-23 21 27 40  https:/...      0   \n",
       "2  hi,\\r \\r great job so far, @saenzramiro ! : \\r...      1   \n",
       "\n",
       "                                                text  \n",
       "0  y-zoom piano roll a y-zoom on the piano roll w...  \n",
       "1  buggy behavior in selection ! screenshot from ...  \n",
       "2  auto update feature hi,\\r \\r great job so far,...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y-zoom piano roll</td>\n",
       "      <td>a y-zoom on the piano roll would be useful.</td>\n",
       "      <td>1</td>\n",
       "      <td>y-zoom piano roll a y-zoom on the piano roll w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buggy behavior in selection</td>\n",
       "      <td>! screenshot from 2016-02-23 21 27 40  https:/...</td>\n",
       "      <td>0</td>\n",
       "      <td>buggy behavior in selection ! screenshot from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auto update feature</td>\n",
       "      <td>hi,\\r \\r great job so far, @saenzramiro ! : \\r...</td>\n",
       "      <td>1</td>\n",
       "      <td>auto update feature hi,\\r \\r great job so far,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:29:34.276658Z",
     "start_time": "2024-10-01T05:29:34.265912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Check the percetage of data points in each category\n",
    "\n",
    "(train_df.label.value_counts(normalize=True).sort_index())*100"
   ],
   "id": "68bd01fd862aea95",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    44.551333\n",
       "1    46.070667\n",
       "2     9.378000\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:29:43.504265Z",
     "start_time": "2024-10-01T05:29:43.473568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label_counts = train_df.label.value_counts(normalize=True).sort_index()\n",
    "plt.figure(figsize=(7,6))\n",
    "label_counts.plot(kind='bar', color=['r','g','b'])\n",
    "\n",
    "B = mpatches.Patch(color='r', label='Bug')\n",
    "F = mpatches.Patch(color='g', label='Feature')\n",
    "Q = mpatches.Patch(color='b', label='Question')\n",
    "\n",
    "plt.legend(handles=[B,F,Q], loc='best')\n",
    "\n",
    "plt.xlabel('Type of Labels')\n",
    "plt.ylabel('Count of Data per Label Category')\n",
    "plt.title('Distribution of labels')\n",
    "plt.show()"
   ],
   "id": "ac132a4b7a6114be",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m label_counts \u001B[38;5;241m=\u001B[39m train_df\u001B[38;5;241m.\u001B[39mlabel\u001B[38;5;241m.\u001B[39mvalue_counts(normalize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39msort_index()\n\u001B[1;32m----> 2\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m7\u001B[39m,\u001B[38;5;241m6\u001B[39m))\n\u001B[0;32m      3\u001B[0m label_counts\u001B[38;5;241m.\u001B[39mplot(kind\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbar\u001B[39m\u001B[38;5;124m'\u001B[39m, color\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mg\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m      5\u001B[0m B \u001B[38;5;241m=\u001B[39m mpatches\u001B[38;5;241m.\u001B[39mPatch(color\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBug\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'plt' is not defined"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:29:56.414556Z",
     "start_time": "2024-10-01T05:29:56.282055Z"
    }
   },
   "cell_type": "code",
   "source": "pd.DataFrame(train_df.text.value_counts())",
   "id": "351992178513f335",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                    count\n",
       "text                                                     \n",
       "y-zoom piano roll a y-zoom on the piano roll wo...      1\n",
       "need mechanism for plugins to contribute compre...      1\n",
       "command palette: generate toc  gfm github flavo...      1\n",
       "difficulty filter for extra exercises is broken...      1\n",
       "ghostauth redirect uri mismatch error for urls ...      1\n",
       "...                                                   ...\n",
       "support map and lambda test.py:\\r    python\\r a...      1\n",
       "api authentication is not available when login_...      1\n",
       "microsoft ad size ignored hi there,\\r \\r     te...      1\n",
       "employee strike functionality initially i thoug...      1\n",
       "ignore headings in code sections do not process...      1\n",
       "\n",
       "[150000 rows x 1 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>y-zoom piano roll a y-zoom on the piano roll would be useful.</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need mechanism for plugins to contribute compressed static resources currently there is no way for plugins to contribute javascript or css which will be compressed along with app-specific and  clld  resources when deployed in production.</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>command palette: generate toc  gfm github flavored markdown creates html anchors for headers and this makes a toc possible. with this command, users will be able to create a toc automatically. it will be inserted where the cursor is.  when generating anchor name, github uses header's text and sanitizes it in some way. we have to find out the details of that sanitization in order to make the feature robust.</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>difficulty filter for extra exercises is broken difficulty filter for extra exercises on the bottom of track pages seems to be broken/buggy.\\r \\r filtering by difficulty either doesn't work at all or yields incorrect results e.g. easy -&gt; nothing, medium &amp; hard -&gt; easy</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ghostauth redirect uri mismatch error for urls with a trailing slash. if you are using ghostauth, and have the url in your config.development.json file configured with a trailing slash on the end. e.g.  http://localhost:2368/blog/  you will not be able to sign in, instead getting a redirect uri mismatch error.\\r \\r     steps to reproduce\\r \\r 1. make sure you have a config.development.json in the root of your ghost dev env\\r 2. inside the config.development.json make sure that you have 2 things:\\r    1. a url that   ends in a trailing slash  , and \\r    2. an auth config that results in   ghostauth   rather than password auth.\\r 3. delete your db file &amp; run  knex-migrator init  to ensure you're working with a fresh db\\r 4. start ghost, navigate to  /ghost/  to get the setup screens\\r 5. on screen 2, hit \\ sign in with ghost\\ \\r 6. see a redirect uri mismatch error\\r \\r \\r     technical details:\\r \\r   ghost version: 1.0.0-alpha.7\\r   node version: 4.4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support map and lambda test.py:\\r    python\\r a = map lambda x: x + 1,  1, 2, 3  \\r    \\r when i try to convert  test.py , the program throws the following error:\\r    \\r traceback  most recent call last :\\r   file \\ /usr/local/bin/pseudo-python\\ , line 11, in &lt;module&gt;\\r     sys.exit main   \\r   file \\ /usr/local/lib/python3.5/site-packages/pseudo_python/main.py\\ , line 61, in main\\r     node = pseudo_python.translate source \\r   file \\ /usr/local/lib/python3.5/site-packages/pseudo_python/__init__.py\\ , line 5, in translate\\r     return pseudo_python.ast_translator.asttranslator pseudo_python.parser.parse source , source .translate  \\r   file \\ /usr/local/lib/python3.5/site-packages/pseudo_python/ast_translator.py\\ , line 104, in translate\\r     main = self._translate_main  \\r   file \\ /usr/local/lib/python3.5/site-packages/pseudo_python/ast_translator.py\\ , line 142, in _translate_main\\r     return self._translate_node self.main \\r   file \\ /usr/local/lib/python3.5/site-packages/pseudo_python/ast_translator.py\\ , line 280, in _translate_node\\r     x = self._translate_node n \\r   file \\ /usr/local/lib/python3.5/site-packages/pseudo_python/ast_translator.py\\ , line 276, in _translate_node\\r     return getattr self, '_translate_%s' % type node .__name__.lower      fields \\r   file \\ /usr/local/lib/python3.5/site-packages/pseudo_python/ast_translator.py\\ , line 969, in _translate_assign\\r     value_node = self._translate_node value \\r   file \\ /usr/local/lib/python3.5/site-packages/pseudo_python/ast_translator.py\\ , line 276, in _translate_node\\r     return getattr self, '_translate_%s' % type node .__name__.lower      fields \\r   file \\ /usr/local/lib/python3.5/site-packages/pseudo_python/ast_translator.py\\ , line 422, in _translate_call\\r     arg_nodes =  arg if not isinstance arg, ast.ast  else self._translate_node arg  for arg in args \\r   file \\ /usr/local/lib/python3.5/site-packages/pseudo_python/ast_translator.py\\ , line 422, in &lt;listcomp&gt;\\r     arg_nodes =  arg if not isinstance arg, ast.ast  else self._translate_node arg  for arg in args \\r   file \\ /usr/local/lib/python3.5/site-packages/pseudo_python/ast_translator.py\\ , line 276, in _translate_node\\r     return getattr self, '_translate_%s' % type node .__name__.lower      fields \\r attributeerror: 'asttranslator' object has no attribute '_translate_lambda'\\r    \\r</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>api authentication is not available when login_required is true the rest api documentation is missing information on how to supply authentication credentials.</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft ad size ignored hi there,\\r \\r     terraform version\\r terraform v0.10.7\\r \\r     affected resource s \\r aws_directory_service_directory\\r \\r     terraform configuration files\\r    hcl\\r resource \\ aws_directory_service_directory\\  \\ active-directory\\  {\\r   name     = \\ ${var.name}\\ \\r   password = \\ ${var.password}\\ \\r   type     = \\ microsoftad\\ \\r   size     = \\ small\\ \\r \\r   vpc_settings {\\r     vpc_id     = \\ ${var.vpc_id}\\ \\r     subnet_ids =  \\ ${var.subnet1}\\ , \\ ${var.subnet2}\\  \\r   }\\r }\\r    \\r \\r     expected behavior\\r i would expect a microsoft ad to be created on aws with a size of standard\\r \\r     actual behavior\\r microsoft ad is created with a size of enterprise\\r \\r     steps to reproduce\\r 1.  terraform apply \\r \\r</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>employee strike functionality initially i thought this card hasn't been fully implemented. but the card automation spreadsheet says full functionality. so i'm unsure whether if this is a bug or not, but figured i'll just report it.\\r \\r as i play employee strike a lot, i've had to resolve a lot of effects manually. the most prominent example is haarpsichord studios.\\r 1 - runner makes a run and steal an agenda\\r 2 - employee strike is played\\r 3 - runner accesses another agenda  same turn  but \\ you've access but cannot steal\\  prompt is shown\\r \\r other examples include:\\r cybernetics division - hand size has to be manually added.\\r tennin institute - corp still gets prompt when their turn begins.\\r \\r it works fine with identities like spark agency, weyland: building a better world, and jinteki: pe.</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ignore headings in code sections do not process headings inside quotes and code sections.</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows × 1 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:30:04.055183Z",
     "start_time": "2024-10-01T05:30:04.033304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Total count of standard stop words list from SpaCy :\",len(STOP_WORDS))\n",
    "print(\"\\nStandard stop words list from SpaCy :\\n\", STOP_WORDS)"
   ],
   "id": "cdc61d99e1c6814f",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STOP_WORDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotal count of standard stop words list from SpaCy :\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;28mlen\u001B[39m(STOP_WORDS))\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mStandard stop words list from SpaCy :\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, STOP_WORDS)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'STOP_WORDS' is not defined"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:30:16.098763Z",
     "start_time": "2024-10-01T05:30:16.079295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#add some redundant words like 'elif' in the stop words list\n",
    "STOP_WORDS = STOP_WORDS + ['elif']\n",
    "print('elif' in STOP_WORDS)\n",
    "\n",
    "#Discard negative words like 'not'and 'no' from this list if required\n",
    "\n",
    "#STOP_WORDS.remove('not')\n",
    "#STOP_WORDS.discard('no')\n",
    "#print(len(STOP_WORDS))"
   ],
   "id": "158124bdf2117904",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STOP_WORDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#add some redundant words like 'elif' in the stop words list\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m STOP_WORDS \u001B[38;5;241m=\u001B[39m STOP_WORDS \u001B[38;5;241m+\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124melif\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124melif\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m STOP_WORDS)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'STOP_WORDS' is not defined"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Creating a sigle Generic Class for text cleaning.\n",
    "\n",
    "class PreprocessText:\n",
    "    \n",
    "    #cleaning abbreviated words\n",
    "    @staticmethod\n",
    "    def remove_contractions(data):\n",
    "        data = re.sub(r\"he's\", \"he is\", data)\n",
    "        data = re.sub(r\"there's\", \"there is\", data)\n",
    "        data = re.sub(r\"We're\", \"We are\", data)\n",
    "        data = re.sub(r\"That's\", \"That is\", data)\n",
    "        data = re.sub(r\"won't\", \"will not\", data)\n",
    "        data = re.sub(r\"they're\", \"they are\", data)\n",
    "        data = re.sub(r\"Can't\", \"Cannot\", data)\n",
    "        data = re.sub(r\"wasn't\", \"was not\", data)\n",
    "        data = re.sub(r\"don\\x89Ûªt\", \"do not\", data)\n",
    "        data= re.sub(r\"aren't\", \"are not\", data)\n",
    "        data = re.sub(r\"isn't\", \"is not\", data)\n",
    "        data = re.sub(r\"What's\", \"What is\", data)\n",
    "        data = re.sub(r\"haven't\", \"have not\", data)\n",
    "        data = re.sub(r\"hasn't\", \"has not\", data)\n",
    "        data = re.sub(r\"There's\", \"There is\", data)\n",
    "        data = re.sub(r\"He's\", \"He is\", data)\n",
    "        data = re.sub(r\"It's\", \"It is\", data)\n",
    "        data = re.sub(r\"You're\", \"You are\", data)\n",
    "        data = re.sub(r\"I'M\", \"I am\", data)\n",
    "        data = re.sub(r\"shouldn't\", \"should not\", data)\n",
    "        data = re.sub(r\"wouldn't\", \"would not\", data)\n",
    "        data = re.sub(r\"i'm\", \"I am\", data)\n",
    "        data = re.sub(r\"I\\x89Ûªm\", \"I am\", data)\n",
    "        data = re.sub(r\"I'm\", \"I am\", data)\n",
    "        data = re.sub(r\"Isn't\", \"is not\", data)\n",
    "        data = re.sub(r\"Here's\", \"Here is\", data)\n",
    "        data = re.sub(r\"you've\", \"you have\", data)\n",
    "        data = re.sub(r\"you\\x89Ûªve\", \"you have\", data)\n",
    "        data = re.sub(r\"we're\", \"we are\", data)\n",
    "        data = re.sub(r\"what's\", \"what is\", data)\n",
    "        data = re.sub(r\"couldn't\", \"could not\", data)\n",
    "        data = re.sub(r\"we've\", \"we have\", data)\n",
    "        data = re.sub(r\"it\\x89Ûªs\", \"it is\", data)\n",
    "        data = re.sub(r\"doesn\\x89Ûªt\", \"does not\", data)\n",
    "        data = re.sub(r\"It\\x89Ûªs\", \"It is\", data)\n",
    "        data = re.sub(r\"Here\\x89Ûªs\", \"Here is\", data)\n",
    "        data = re.sub(r\"who's\", \"who is\", data)\n",
    "        data = re.sub(r\"I\\x89Ûªve\", \"I have\", data)\n",
    "        data = re.sub(r\"y'all\", \"you all\", data)\n",
    "        data = re.sub(r\"can\\x89Ûªt\", \"cannot\", data)\n",
    "        data = re.sub(r\"would've\", \"would have\", data)\n",
    "        data = re.sub(r\"it'll\", \"it will\", data)\n",
    "        data = re.sub(r\"we'll\", \"we will\", data)\n",
    "        data = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", data)\n",
    "        data = re.sub(r\"We've\", \"We have\", data)\n",
    "        data = re.sub(r\"he'll\", \"he will\", data)\n",
    "        data = re.sub(r\"Y'all\", \"You all\", data)\n",
    "        data = re.sub(r\"Weren't\", \"Were not\", data)\n",
    "        data = re.sub(r\"Didn't\", \"Did not\", data)\n",
    "        data = re.sub(r\"they'll\", \"they will\", data)\n",
    "        data = re.sub(r\"they'd\", \"they would\", data)\n",
    "        data = re.sub(r\"DON'T\", \"DO NOT\", data)\n",
    "        data = re.sub(r\"That\\x89Ûªs\", \"That is\", data)\n",
    "        data = re.sub(r\"they've\", \"they have\", data)\n",
    "        data = re.sub(r\"i'd\", \"I would\", data)\n",
    "        data = re.sub(r\"should've\", \"should have\", data)\n",
    "        data = re.sub(r\"You\\x89Ûªre\", \"You are\", data)\n",
    "        data = re.sub(r\"where's\", \"where is\", data)\n",
    "        data = re.sub(r\"Don\\x89Ûªt\", \"Do not\", data)\n",
    "        data = re.sub(r\"we'd\", \"we would\", data)\n",
    "        data = re.sub(r\"i'll\", \"I will\", data)\n",
    "        data = re.sub(r\"weren't\", \"were not\", data)\n",
    "        data = re.sub(r\"They're\", \"They are\", data)\n",
    "        data = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", data)\n",
    "        data = re.sub(r\"you\\x89Ûªll\", \"you will\", data)\n",
    "        data = re.sub(r\"I\\x89Ûªd\", \"I would\", data)\n",
    "        data = re.sub(r\"let's\", \"let us\", data)\n",
    "        data = re.sub(r\"it's\", \"it is\", data)\n",
    "        data = re.sub(r\"can't\", \"cannot\", data)\n",
    "        data = re.sub(r\"don't\", \"do not\", data)\n",
    "        data = re.sub(r\"you're\", \"you are\", data)\n",
    "        data = re.sub(r\"i've\", \"I have\", data)\n",
    "        data = re.sub(r\"that's\", \"that is\", data)\n",
    "        data = re.sub(r\"i'll\", \"I will\", data)\n",
    "        data = re.sub(r\"doesn't\", \"does not\",data)\n",
    "        data = re.sub(r\"i'd\", \"I would\", data)\n",
    "        data = re.sub(r\"didn't\", \"did not\", data)\n",
    "        data = re.sub(r\"ain't\", \"am not\", data)\n",
    "        data = re.sub(r\"you'll\", \"you will\", data)\n",
    "        data = re.sub(r\"I've\", \"I have\", data)\n",
    "        data = re.sub(r\"Don't\", \"do not\", data)\n",
    "        data = re.sub(r\"I'll\", \"I will\", data)\n",
    "        data = re.sub(r\"I'd\", \"I would\", data)\n",
    "        data = re.sub(r\"Let's\", \"Let us\", data)\n",
    "        data = re.sub(r\"you'd\", \"You would\", data)\n",
    "        data = re.sub(r\"It's\", \"It is\", data)\n",
    "        data = re.sub(r\"Ain't\", \"am not\", data)\n",
    "        data = re.sub(r\"Haven't\", \"Have not\", data)\n",
    "        data = re.sub(r\"Could've\", \"Could have\", data)\n",
    "        data = re.sub(r\"youve\", \"you have\", data)  \n",
    "        data = re.sub(r\"donå«t\", \"do not\", data)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "    #cleaning Urls\n",
    "    @staticmethod\n",
    "    def remove_urls(data):\n",
    "        clean_url_regex = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "        data = clean_url_regex.sub(r\" \", data)\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    #cleaning noisy data\n",
    "    @staticmethod\n",
    "    def remove_noisy_char(data):\n",
    "        data = data.replace(\"\\\\r\", \" \").strip()\n",
    "        data = data.replace(\"\\r\", \" \").strip()\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    #cleaning HTML tags\n",
    "    @staticmethod\n",
    "    def remove_HTML_tags(data):\n",
    "        soup = BeautifulSoup(data, 'html.parser') \n",
    "        return soup.get_text()\n",
    "        \n",
    "        \n",
    "    #cleaning emojis\n",
    "    @staticmethod\n",
    "    def remove_emojis(data):\n",
    "        emoji_clean= re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "        \n",
    "        data = emoji_clean.sub(r\" \",data)\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    #cleaning unicode characters\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def remove_unicode_chars(data):\n",
    "        data = (data.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
    "        return data\n",
    "    \"\"\"\n",
    "    \n",
    "    #cleaning punctuations\n",
    "    @staticmethod\n",
    "    def remove_punctuations(data):\n",
    "        \n",
    "        #clean_punct_regex = re.compile(r\"[^\\w\\s\\d]+\")\n",
    "        clean_punct_regex = re.compile(r\"[^a-zA-Z0-9\\s]+\")\n",
    "        data = clean_punct_regex.sub(r\" \", data)\n",
    "                        \n",
    "        #credits - https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n",
    "        #data = data.translate(str.maketrans('', '', string.punctuation))\n",
    "                \n",
    "        return data\n",
    "    \n",
    "    \n",
    "    #cleaning numeric characters\n",
    "    @staticmethod\n",
    "    def remove_numerics(data):\n",
    "        #clean_alphanum_regex = re.compile(r\"\\S*\\d\\S*\")\n",
    "        #data = clean_alphanum_regex.sub(r\"\", data)\n",
    "        \n",
    "        clean_num_regex = re.compile(r\"\\b[0-9]+\\b\")\n",
    "        data = clean_num_regex.sub(r\"\", data)\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    #cleaning single characters\n",
    "    @staticmethod\n",
    "    def remove_single_chars(data):\n",
    "        #credits - https://stackoverflow.com/questions/42066352/python-regex-to-replace-all-single-word-characters-in-string\n",
    "        clean_single_len_regex = re.compile(r\"\\b[a-zA-Z]\\b\")\n",
    "        data = clean_single_len_regex.sub(r\"\", data)\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    #cleaning unwanted whitespaces\n",
    "    @staticmethod\n",
    "    def remove_redundant_whiteSpaces(data):\n",
    "        clean_redundant_whitespaces_regex = re.compile(r\"\\s\\s+\") #check for more consecutive spaces\n",
    "        data = clean_redundant_whitespaces_regex.sub(r\" \", data) #replace with single space\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    #cleaning stopwords\n",
    "    @staticmethod\n",
    "    def remove_stopwords(data):\n",
    "        data = ' '.join(word.lower() for word in data.split() if word.lower() not in STOP_WORDS)\n",
    "        data = data.strip()\n",
    "        return data\n",
    "    \n",
    "    #cleaning long length words greater than 25 chars\n",
    "    @staticmethod\n",
    "    def remove_long_length_tokens(data):\n",
    "        data = ' '.join(word.lower() for word in data.split() if len(word) <= 25)\n",
    "        data = data.strip()\n",
    "        return data\n",
    "    \n",
    "    #lemmatizing the text data\n",
    "    @staticmethod\n",
    "    def lemmatize_corpus(data, method = 'wordnet'):\n",
    "        if method == 'spacy':\n",
    "            out_data = \" \".join([token.lemma_ for token in nlp_spcy(data)])\n",
    "        else:\n",
    "            lemmatizer=WordNetLemmatizer()\n",
    "            out_data = ' '.join(lemmatizer.lemmatize(word) for word in data.split())\n",
    "        \n",
    "        return out_data"
   ],
   "id": "dea77523f974a98f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#apply text pre-processing\n",
    "\n",
    "def text_cleaning(df,col,clean_col):\n",
    "    \n",
    "    df[clean_col]= df[col].apply(PreprocessText.remove_contractions)\n",
    "    df[clean_col]= df[clean_col].apply(PreprocessText.remove_urls)\n",
    "    df[clean_col]= df[clean_col].apply(PreprocessText.remove_noisy_char)\n",
    "    df[clean_col]= df[clean_col].apply(PreprocessText.remove_HTML_tags)\n",
    "    df[clean_col]= df[clean_col].apply(PreprocessText.remove_emojis)\n",
    "    #df[clean_col]= df[clean_col].apply(PreprocessText.remove_unicode_chars)\n",
    "    df[clean_col]= df[clean_col].apply(PreprocessText.remove_punctuations)\n",
    "    df[clean_col]= df[clean_col].apply(PreprocessText.remove_numerics)\n",
    "    df[clean_col]= df[clean_col].apply(PreprocessText.remove_single_chars)\n",
    "    df[clean_col]= df[clean_col].apply(PreprocessText.remove_redundant_whiteSpaces)\n",
    "    df[clean_col]= df[clean_col].apply(PreprocessText.remove_stopwords)\n",
    "    df[clean_col]= df[clean_col].apply(PreprocessText.remove_long_length_tokens)\n",
    "    \n",
    "    return df\n",
    "\n",
    "start_time = time.clock()\n",
    "train_df = text_cleaning(train_df, 'text', 'clean_text')\n",
    "print(\"time required text_cleaning :\", time.clock() - start_time, \"sec.\")"
   ],
   "id": "c08f34041b2d46e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "i = 4\n",
    "#check the text data before text-preprocessing\n",
    "print(f\"text data at index {i} before text pre-processing : \\n\\n {train_df.text.iloc[i]}\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "#check the cleaned text data post text-preprocessing\n",
    "print(f\"text data at index {i} post text pre-processing : \\n\\n {train_df.clean_text.iloc[i]}\")"
   ],
   "id": "f084d352feeb6b55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#apply lemmatization\n",
    "start_time = time.clock()\n",
    "train_df['clean_text'] = train_df['clean_text'].apply(PreprocessText.lemmatize_corpus, method='wordnet')\n",
    "print(\"time required lemmatizing text :\", time.clock() - start_time, \"sec.\\n\")\n",
    "\n",
    "#check the cleaned text data post Lemmatization\n",
    "print(f\"text data at index {i} post text Lemmatization : \\n\\n {train_df.clean_text.iloc[i]}\")"
   ],
   "id": "7b2d8929d48a0ef6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('os')"
   ],
   "id": "219b508dc69db52f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_df['clean_text'] = train_df['clean_text'].apply(PreprocessText.remove_single_chars)\n",
    "train_df['clean_text'] = train_df['clean_text'].apply(PreprocessText.remove_redundant_whiteSpaces)\n",
    "print(f\"text data at index {i} post single char text removal : \\n\\n {train_df.clean_text.iloc[i]}\")"
   ],
   "id": "88b0f1ed8e5f0746"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def BOW(data):\n",
    "    bow_vectorizer = CountVectorizer(lowercase=True,stop_words= stop_words,token_pattern=r'\\w+',ngram_range=(1,2),\n",
    "                                     analyzer='word',max_features=50000)\n",
    "    train_bow = bow_vectorizer.fit_transform(data)\n",
    "    return bow_vectorizer,train_bow"
   ],
   "id": "91316fe82dcdea7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#BOW Vectorized Data\n",
    "\n",
    "bow_vectorizer,train_bow = BOW(train_df.clean_text)"
   ],
   "id": "be1bd26caaf411b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Type of BOW count vectorizer :\",type(train_bow))\n",
    "print(\"Shape of BOW count vectorizer :\",train_bow.get_shape())\n",
    "print(\"Number of unique words (uni-grams and bi-grams) :\", train_bow.get_shape()[1])"
   ],
   "id": "de74bb9ed553c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# document wise count of each n-gram term that appears in the BOW transformed corpus\n",
    "print(train_bow[0:2])"
   ],
   "id": "33278d4a3c9da9d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#check some of the created features using BOW approach\n",
    "\n",
    "print(\"some sample features(unique words in the corpus) :\\n\",bow_vectorizer.get_feature_names()[10000:10020])"
   ],
   "id": "cfa60dd06816448c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Number of times a particular n-gram feature appears in the corpus using BOW vectorization\n",
    "\n",
    "list(zip(bow_vectorizer.get_feature_names()[10000:10020], train_bow.sum(0).getA1()[10000:10020]))"
   ],
   "id": "e599998029eef064"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Top words in BOW embedding\n",
    "\n",
    "print(dict(itertools.islice(bow_vectorizer.vocabulary_.items(), 25)))\n",
    "#print(dict(list(bow_vectorizer.vocabulary_.items())[0: 25]) )"
   ],
   "id": "a666add8868a07c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:32:06.617310Z",
     "start_time": "2024-10-01T05:32:06.612600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tfidf(data):\n",
    "    tfidf_vectorizer = TfidfVectorizer(lowercase=True,stop_words=stop_words,analyzer='word',\n",
    "                                       token_pattern=r'\\w+',ngram_range=(1,2),min_df= 3,\n",
    "                                       max_features=50000,use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "    train_tfidf = tfidf_vectorizer.fit_transform(data)\n",
    "    return tfidf_vectorizer,train_tfidf"
   ],
   "id": "1b05878e2611dee9",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:32:14.134475Z",
     "start_time": "2024-10-01T05:32:14.108496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TFIDF Vectorized Data\n",
    "\n",
    "tfidf_vectorizer,train_tfidf = tfidf(train_df.clean_text)"
   ],
   "id": "f647089c533a22a1",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'clean_text'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#TFIDF Vectorized Data\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m tfidf_vectorizer,train_tfidf \u001B[38;5;241m=\u001B[39m tfidf(train_df\u001B[38;5;241m.\u001B[39mclean_text)\n",
      "File \u001B[1;32m~\\.conda\\envs\\SDPtest\\Lib\\site-packages\\pandas\\core\\generic.py:6299\u001B[0m, in \u001B[0;36mNDFrame.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   6292\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   6293\u001B[0m     name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_internal_names_set\n\u001B[0;32m   6294\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_metadata\n\u001B[0;32m   6295\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_accessors\n\u001B[0;32m   6296\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info_axis\u001B[38;5;241m.\u001B[39m_can_hold_identifiers_and_holds_name(name)\n\u001B[0;32m   6297\u001B[0m ):\n\u001B[0;32m   6298\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m[name]\n\u001B[1;32m-> 6299\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mobject\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getattribute__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'clean_text'"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:32:24.506177Z",
     "start_time": "2024-10-01T05:32:24.482187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Type of TFIDF count vectorizer :\",type(train_tfidf))\n",
    "print(\"Shape of TFIDF count vectorizer :\",train_tfidf.get_shape())\n",
    "print(\"Number of unique words including uni-grams and bi-grams  :\", train_tfidf.get_shape()[1])"
   ],
   "id": "ab42d152827bf576",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mType of TFIDF count vectorizer :\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;28mtype\u001B[39m(train_tfidf))\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShape of TFIDF count vectorizer :\u001B[39m\u001B[38;5;124m\"\u001B[39m,train_tfidf\u001B[38;5;241m.\u001B[39mget_shape())\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of unique words including uni-grams and bi-grams  :\u001B[39m\u001B[38;5;124m\"\u001B[39m, train_tfidf\u001B[38;5;241m.\u001B[39mget_shape()[\u001B[38;5;241m1\u001B[39m])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'train_tfidf' is not defined"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# document wise TF-IDF score of each n-gram term that appears in the TFIDF transformed corpus\n",
    "\n",
    "print(train_tfidf[0:2])"
   ],
   "id": "3b97993aee6380a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:32:37.246898Z",
     "start_time": "2024-10-01T05:32:37.229838Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"some sample features(unique words) in the TF-IDF vectorized corpus :\\n\\n\",tfidf_vectorizer.get_feature_names()[10000:10020])",
   "id": "811ab2de9b5245cc",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msome sample features(unique words) in the TF-IDF vectorized corpus :\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,tfidf_vectorizer\u001B[38;5;241m.\u001B[39mget_feature_names()[\u001B[38;5;241m10000\u001B[39m:\u001B[38;5;241m10020\u001B[39m])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'tfidf_vectorizer' is not defined"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Total TF-IDF score of each n-gram term that appears in the entire TFIDF transformed corpus \n",
    "\n",
    "list(zip(tfidf_vectorizer.get_feature_names()[10000:10020], train_tfidf.sum(0).getA1()[10000:10020]))"
   ],
   "id": "cbdfc26e7613d75c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Top words in TF_IDF embedding\n",
    "\n",
    "print(dict(itertools.islice(tfidf_vectorizer.vocabulary_.items(), 25)))\n",
    "#print(dict(list(tfidf_vectorizer.vocabulary_.items())[0: 25]) )"
   ],
   "id": "5eb22c8bcb3fb83b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "list_of_sentances = list(train_df.clean_text.str.split())\n",
    "print(f\"First sentence : {list_of_sentances[0]}\")"
   ],
   "id": "e36abdbb36c006ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:33:21.945942Z",
     "start_time": "2024-10-01T05:33:21.925950Z"
    }
   },
   "cell_type": "code",
   "source": "np.sort([len(sent) for sent in list_of_sentances])",
   "id": "2f8681863eafc98a",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_of_sentances' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m np\u001B[38;5;241m.\u001B[39msort([\u001B[38;5;28mlen\u001B[39m(sent) \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m list_of_sentances])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'list_of_sentances' is not defined"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:33:19.081055Z",
     "start_time": "2024-10-01T05:33:19.060380Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.iloc[[[len(sent) for sent in list_of_sentances].index(0)]]",
   "id": "b2ecb95a8508de8",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_of_sentances' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_df\u001B[38;5;241m.\u001B[39miloc[[[\u001B[38;5;28mlen\u001B[39m(sent) \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m list_of_sentances]\u001B[38;5;241m.\u001B[39mindex(\u001B[38;5;241m0\u001B[39m)]]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'list_of_sentances' is not defined"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:33:17.468055Z",
     "start_time": "2024-10-01T05:33:17.462665Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.iloc[146131]",
   "id": "284bc6c16f501368",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title            transient errors when creating file locks\n",
       "body     a  filenotfoundexception  thrown  here  https:...\n",
       "label                                                    0\n",
       "text     transient errors when creating file locks a  f...\n",
       "Name: 146131, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:33:15.830628Z",
     "start_time": "2024-10-01T05:33:15.789235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df.drop([146131],axis=0,inplace=True)\n",
    "train_df.reset_index(drop=True, inplace = True)"
   ],
   "id": "6bb64104223de963",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:33:35.279924Z",
     "start_time": "2024-10-01T05:33:35.261143Z"
    }
   },
   "cell_type": "code",
   "source": "list_of_sentances = [sent for sent in list_of_sentances if len(sent) > 0]",
   "id": "10379419d8d9c00b",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_of_sentances' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m list_of_sentances \u001B[38;5;241m=\u001B[39m [sent \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m list_of_sentances \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(sent) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'list_of_sentances' is not defined"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:33:44.525346Z",
     "start_time": "2024-10-01T05:33:44.497729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# vectorize the words using Word2Vec skip-gram based\n",
    "\n",
    "if os.path.isfile(\"../input/github-w2vec-300-trained/w2v_model.bin\"):\n",
    "    # load the Word2Vec model from saved Word2Vec model\n",
    "    w2v_model = KeyedVectors.load(\"../input/github-w2vec-300-trained/w2v_model.bin\")\n",
    "    print(\"Trained 300-dim word2vec model:\", w2v_model)\n",
    "else:\n",
    "    start_time = time.clock()\n",
    "    w2v_model = Word2Vec(list_of_sentances,vector_size=300,window=8,min_count=5,workers=4,sg=1,hs=0,epochs=10)\n",
    "    print(\"time required for Word2Vec model trainin :\", time.clock() - start_time, \"sec.\")\n",
    "    # save the modeled words produced from Word2Vec\n",
    "    w2v_model.save('w2v_model.bin')"
   ],
   "id": "5401a90301288679",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# vectorize the words using Word2Vec skip-gram based\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../input/github-w2vec-300-trained/w2v_model.bin\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;66;03m# load the Word2Vec model from saved Word2Vec model\u001B[39;00m\n\u001B[0;32m      5\u001B[0m     w2v_model \u001B[38;5;241m=\u001B[39m KeyedVectors\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../input/github-w2vec-300-trained/w2v_model.bin\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrained 300-dim word2vec model:\u001B[39m\u001B[38;5;124m\"\u001B[39m, w2v_model)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'os' is not defined"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:33:56.106072Z",
     "start_time": "2024-10-01T05:33:56.086846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "list_of_words_w2v = list(w2v_model.wv.key_to_index)\n",
    "print(f\"Total number of words in trained word2vec : {len(list_of_words_w2v)}\")\n",
    "print(list_of_words_w2v[:50])"
   ],
   "id": "861b91a9df2face4",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w2v_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m list_of_words_w2v \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(w2v_model\u001B[38;5;241m.\u001B[39mwv\u001B[38;5;241m.\u001B[39mkey_to_index)\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotal number of words in trained word2vec : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(list_of_words_w2v)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(list_of_words_w2v[:\u001B[38;5;241m50\u001B[39m])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'w2v_model' is not defined"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:33:55.129273Z",
     "start_time": "2024-10-01T05:33:55.109749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check for the top-10 most similar words \n",
    "\n",
    "print(w2v_model.wv.most_similar('useful',topn=10))\n",
    "print('=='*50)\n",
    "print(w2v_model.wv.most_similar('buggy', topn=10))"
   ],
   "id": "b388e2ffd5d7d1be",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w2v_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# check for the top-10 most similar words \u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28mprint\u001B[39m(w2v_model\u001B[38;5;241m.\u001B[39mwv\u001B[38;5;241m.\u001B[39mmost_similar(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124museful\u001B[39m\u001B[38;5;124m'\u001B[39m,topn\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m))\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m==\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m50\u001B[39m)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(w2v_model\u001B[38;5;241m.\u001B[39mwv\u001B[38;5;241m.\u001B[39mmost_similar(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbuggy\u001B[39m\u001B[38;5;124m'\u001B[39m, topn\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'w2v_model' is not defined"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#View the embedding vector for word 'buggy' and 'useful' using Word2Vec\n",
    "\n",
    "print(w2v_model)\n",
    "print(type(w2v_model))\n",
    "# print(\"buggy:\",w2v_model.wv['buggy'])\n",
    "print(\"300-dim vector repreentation for word 'useful' using word2vec embeddings:\\n\",w2v_model.wv['useful'])"
   ],
   "id": "6c44a9c720ef19d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Measure Cosine distance\n",
    "\n",
    "distance = w2v_model.wv.similarity('useful','buggy')\n",
    "print(distance)"
   ],
   "id": "fe9cdea9138d6f87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#plot the embedding vector for word 'buggy\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(w2v_model.wv['buggy'])\n",
    "plt.plot(w2v_model.wv['useful'])\n",
    "plt.show()"
   ],
   "id": "98d3c1e9c1517c8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "# Visualization of word2Vec embedded words in 2D using T-SNE transform  \n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "w2v_data = w2v_model.wv[w2v_model.wv.key_to_index]\n",
    "w2v_tsne_transformed = tsne.fit_transform(w2v_data[:100])\n",
    "\n",
    "# create a scatter plot for projecting glove vetors in 2D\n",
    "\n",
    "fig = px.scatter(x=w2v_tsne_transformed[:,0], y=w2v_tsne_transformed[:,1],text=list_of_words_w2v[:100])\n",
    "fig.update_traces(textposition='bottom center')\n",
    "fig.update_layout(\n",
    "     width=1100,\n",
    "    height=900,\n",
    "    title_text='Visualization of Word2Vec embedded words in 2D using T-SNE transform'\n",
    ")\n",
    "fig.show()"
   ],
   "id": "21311cd14e9586f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "50cf8217c52732f4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
